{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d689e2f-8d0c-47df-a506-7ce2ce931a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from datetime import datetime\n",
    "import streamlit as st\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed4017-c5c2-448a-9a4f-8e75c1143a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata\n",
    "        df = pd.read_csv('metadata.csv')       \n",
    "# Load the data\n",
    "df = load_metadata()\n",
    "       sample_data = {\n",
    "        'cord_uid': [f'uid_{i}' for i in range(1000)],\n",
    "        'title': [f'COVID-19 Research Paper {i} on various topics' for i in range(1000)],\n",
    "        'abstract': [f'This is abstract {i} discussing COVID-19 pandemic impacts.' for i in range(1000)],\n",
    "        'publish_time': np.random.choice(dates, 1000),\n",
    "        'journal': np.random.choice(journals + [None] * 2, 1000),\n",
    "        'authors': [f'Author {i}, Co-author {i}' for i in range(1000)],\n",
    "        'url': [f'https://example.com/paper{i}' for i in range(1000)],\n",
    "        'source_x': np.random.choice(['PubMed', 'PMC', 'WHO', 'CDC'], 1000)\n",
    "    }\n",
    "    df = pd.DataFrame(sample_data)\n",
    "\n",
    "# Examine the data\n",
    "print(\"=== First few rows ===\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n=== DataFrame info ===\")\n",
    "print(f\"Dimensions: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "print(\"\\n=== Column names ===\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\n=== Data types ===\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n=== Missing values ===\")\n",
    "missing_data = df.isnull().sum()\n",
    "print(missing_data[missing_data > 0])\n",
    "\n",
    "print(\"\\n=== Basic statistics ===\")\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c09f3113-ef19-4735-bb74-04498df37918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_analysis(df):\n",
    "    \"\"\"Perform comprehensive data analysis\"\"\"\n",
    "    \n",
    "    print(\"=== PERFORMING DATA ANALYSIS ===\")\n",
    "    \n",
    "    # 1. Papers by publication year\n",
    "    print(\"\\n1. Papers by publication year:\")\n",
    "    yearly_counts = df['publication_year'].value_counts().sort_index()\n",
    "    print(yearly_counts)\n",
    "    \n",
    "    # 2. Top journals\n",
    "    print(\"\\n2. Top journals publishing COVID-19 research:\")\n",
    "    top_journals = df['journal'].value_counts().head(10)\n",
    "    print(top_journals)\n",
    "    \n",
    "    # 3. Most frequent words in titles\n",
    "    print(\"\\n3. Most frequent words in titles:\")\n",
    "    all_titles = ' '.join(df['title_clean'].dropna())\n",
    "    words = re.findall(r'\\b\\w+\\b', all_titles)\n",
    "    \n",
    "    # Remove common stop words\n",
    "    stop_words = {'the', 'and', 'of', 'in', 'to', 'a', 'for', 'with', 'on', 'as', 'by', \n",
    "                 'an', 'at', 'from', 'that', 'is', 'are', 'this', 'these', 'those', 'or'}\n",
    "    \n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words and len(word) > 2]\n",
    "    word_freq = Counter(filtered_words).most_common(20)\n",
    "    print(\"Top 20 words in titles:\")\n",
    "    for word, count in word_freq:\n",
    "        print(f\"{word}: {count}\")\n",
    "    \n",
    "    return yearly_counts, top_journals, word_freq\n",
    "\n",
    "def create_visualizations(df, yearly_counts, top_journals, word_freq):\n",
    "    \"\"\"Create comprehensive visualizations\"\"\"\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Publications over time\n",
    "    axes[0, 0].plot(yearly_counts.index, yearly_counts.values, marker='o', linewidth=2)\n",
    "    axes[0, 0].set_title('COVID-19 Publications Over Time', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Year')\n",
    "    axes[0, 0].set_ylabel('Number of Publications')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Top journals\n",
    "    top_10_journals = top_journals.head(10)\n",
    "    axes[0, 1].barh(range(len(top_10_journals)), top_10_journals.values)\n",
    "    axes[0, 1].set_yticks(range(len(top_10_journals)))\n",
    "    axes[0, 1].set_yticklabels(top_10_journals.index)\n",
    "    axes[0, 1].set_title('Top 10 Journals by Publication Count', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Number of Publications')\n",
    "    \n",
    "    # 3. Word cloud\n",
    "    all_titles = ' '.join(df['title_clean'].dropna())\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                         max_words=100).generate(all_titles)\n",
    "    axes[1, 0].imshow(wordcloud, interpolation='bilinear')\n",
    "    axes[1, 0].set_title('Word Cloud of Paper Titles', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # 4. Distribution of papers by source (if available)\n",
    "    if 'source_x' in df.columns:\n",
    "        source_counts = df['source_x'].value_counts().head(8)\n",
    "        axes[1, 1].pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%')\n",
    "        axes[1, 1].set_title('Paper Distribution by Source', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cord19_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional visualization: Abstract word count distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df[df['abstract_word_count'] > 0]['abstract_word_count'], bins=50, alpha=0.7)\n",
    "    plt.title('Distribution of Abstract Word Count', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Word Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c127b3-d257-432e-88cc-97aa6f29a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Streamlit app as a separate file: app.py\n",
    "\n",
    "\"\"\"\n",
    "# CORD-19 Data Explorer App (app.py)\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "\n",
    "# Set page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"CORD-19 Data Explorer\",\n",
    "    page_icon=\"ðŸ“Š\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Title and description\n",
    "st.title(\"CORD-19 COVID-19 Research Data Explorer\")\n",
    "st.write(\"\"\"\n",
    "This interactive dashboard explores the COVID-19 Open Research Dataset (CORD-19), \n",
    "containing scientific papers about COVID-19 and related coronaviruses.\n",
    "\"\"\")\n",
    "\n",
    "# Sidebar for controls\n",
    "st.sidebar.header(\"Controls\")\n",
    "st.sidebar.write(\"Adjust the parameters to explore the data:\")\n",
    "\n",
    "# Load data (in a real app, you'd load your actual cleaned data)\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    # This would load your actual cleaned DataFrame\n",
    "    # For demo purposes, we'll use the sample data creation logic\n",
    "    return df_clean\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "# Year range selector\n",
    "if 'publication_year' in df.columns:\n",
    "    min_year = int(df['publication_year'].min())\n",
    "    max_year = int(df['publication_year'].max())\n",
    "    \n",
    "    year_range = st.sidebar.slider(\n",
    "        \"Select Year Range\",\n",
    "        min_value=min_year,\n",
    "        max_value=max_year,\n",
    "        value=(min_year, max_year)\n",
    "    )\n",
    "    \n",
    "    # Filter data based on selection\n",
    "    filtered_df = df[\n",
    "        (df['publication_year'] >= year_range[0]) & \n",
    "        (df['publication_year'] <= year_range[1])\n",
    "    ]\n",
    "else:\n",
    "    filtered_df = df\n",
    "\n",
    "# Journal selector\n",
    "if 'journal' in df.columns:\n",
    "    journals = ['All'] + sorted(df['journal'].unique().tolist())\n",
    "    selected_journal = st.sidebar.selectbox(\"Select Journal\", journals)\n",
    "    \n",
    "    if selected_journal != 'All':\n",
    "        filtered_df = filtered_df[filtered_df['journal'] == selected_journal]\n",
    "\n",
    "# Main content area\n",
    "col1, col2 = st.columns([2, 1])\n",
    "\n",
    "with col1:\n",
    "    st.subheader(\"Key Metrics\")\n",
    "    \n",
    "    # Display metrics\n",
    "    metric_col1, metric_col2, metric_col3, metric_col4 = st.columns(4)\n",
    "    \n",
    "    with metric_col1:\n",
    "        st.metric(\"Total Papers\", len(filtered_df))\n",
    "    \n",
    "    with metric_col2:\n",
    "        if 'publication_year' in filtered_df.columns:\n",
    "            st.metric(\"Latest Year\", int(filtered_df['publication_year'].max()))\n",
    "    \n",
    "    with metric_col3:\n",
    "        if 'has_abstract' in filtered_df.columns:\n",
    "            abstracts_available = filtered_df['has_abstract'].sum()\n",
    "            st.metric(\"Abstracts Available\", abstracts_available)\n",
    "    \n",
    "    with metric_col4:\n",
    "        if 'journal' in filtered_df.columns:\n",
    "            unique_journals = filtered_df['journal'].nunique()\n",
    "            st.metric(\"Unique Journals\", unique_journals)\n",
    "\n",
    "# Visualizations\n",
    "st.subheader(\"Data Visualizations\")\n",
    "\n",
    "# Create tabs for different visualizations\n",
    "tab1, tab2, tab3, tab4 = st.tabs([\n",
    "    \"Publications Over Time\", \n",
    "    \"Top Journals\", \n",
    "    \"Title Word Cloud\",\n",
    "    \"Data Sample\"\n",
    "])\n",
    "\n",
    "with tab1:\n",
    "    st.write(\"### COVID-19 Publications Over Time\")\n",
    "    if 'publication_year' in filtered_df.columns:\n",
    "        yearly_counts = filtered_df['publication_year'].value_counts().sort_index()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.plot(yearly_counts.index, yearly_counts.values, marker='o', linewidth=2)\n",
    "        ax.set_xlabel('Year')\n",
    "        ax.set_ylabel('Number of Publications')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_title('Publications Over Time')\n",
    "        st.pyplot(fig)\n",
    "    else:\n",
    "        st.write(\"Publication year data not available.\")\n",
    "\n",
    "with tab2:\n",
    "    st.write(\"### Top Journals\")\n",
    "    if 'journal' in filtered_df.columns:\n",
    "        top_journals = filtered_df['journal'].value_counts().head(10)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.barh(range(len(top_journals)), top_journals.values)\n",
    "        ax.set_yticks(range(len(top_journals)))\n",
    "        ax.set_yticklabels(top_journals.index)\n",
    "        ax.set_xlabel('Number of Publications')\n",
    "        ax.set_title('Top 10 Journals')\n",
    "        st.pyplot(fig)\n",
    "    else:\n",
    "        st.write(\"Journal data not available.\")\n",
    "\n",
    "with tab3:\n",
    "    st.write(\"### Word Cloud of Paper Titles\")\n",
    "    if 'title_clean' in filtered_df.columns:\n",
    "        all_titles = ' '.join(filtered_df['title_clean'].dropna())\n",
    "        \n",
    "        if all_titles.strip():\n",
    "            wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                                max_words=100).generate(all_titles)\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax.imshow(wordcloud, interpolation='bilinear')\n",
    "            ax.axis('off')\n",
    "            ax.set_title('Word Cloud of Paper Titles')\n",
    "            st.pyplot(fig)\n",
    "        else:\n",
    "            st.write(\"No title data available for word cloud.\")\n",
    "    else:\n",
    "        st.write(\"Title data not available.\")\n",
    "\n",
    "with tab4:\n",
    "    st.write(\"### Sample Data\")\n",
    "    st.dataframe(filtered_df.head(100))\n",
    "\n",
    "# Additional information\n",
    "st.sidebar.subheader(\"â„¹ï¸ About\")\n",
    "st.sidebar.write(\"\"\"\n",
    "This app analyzes the CORD-19 dataset containing COVID-19 research papers.\n",
    "\n",
    "**Data Source:** COVID-19 Open Research Dataset (CORD-19)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdd0cd58-bce9-4964-8559-b964aacbb63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    PROJECT REFLECTION:\n",
      "    \n",
      "    SUCCESSES:\n",
      "    1. Complete pipeline implementation from data loading to interactive app\n",
      "    2. Comprehensive data cleaning and preparation\n",
      "    3. Multiple visualization types for different insights\n",
      "    4. User-friendly Streamlit interface\n",
      "    \n",
      "    CHALLENGES:\n",
      "    1. Handling large dataset efficiently\n",
      "    2. Dealing with inconsistent data quality\n",
      "    3. Choosing appropriate visualizations\n",
      "    4. Balancing complexity with usability\n",
      "    \n",
      "    KEY LEARNINGS:\n",
      "    1. Importance of thorough data exploration before analysis\n",
      "    2. Value of interactive tools for data discovery\n",
      "    3. Need for flexible data cleaning approaches\n",
      "    4. Benefits of modular code organization\n",
      "    \n",
      "    AREAS FOR IMPROVEMENT:\n",
      "    1. Add more advanced text analysis (sentiment, topics)\n",
      "    2. Implement performance optimizations for larger datasets\n",
      "    3. Add more interactive filtering options\n",
      "    4. Include statistical testing for observed patterns\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# documentation_report.md\n",
    "\n",
    "\"\"\"\n",
    "# CORD-19 Data Analysis Project - Documentation and Reflection\n",
    "\n",
    "## Project Overview\n",
    "This project analyzes the CORD-19 dataset, which contains metadata about COVID-19 research papers. The analysis includes data loading, cleaning, exploration, visualization, and an interactive Streamlit application.\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### 1. Data Characteristics\n",
    "- **Dataset Size**: [Number] rows, [Number] columns\n",
    "- **Time Period**: Papers published from [Start Year] to [End Year]\n",
    "- **Key Columns**: Title, Abstract, Publication Date, Journal, Authors\n",
    "\n",
    "### 2. Publication Trends\n",
    "- **Peak Publication Year**: [Year] with [Number] papers\n",
    "- **Growth Pattern**: [Description of publication growth over time]\n",
    "\n",
    "### 3. Journal Distribution\n",
    "- **Top Journals**: [List top 3-5 journals]\n",
    "- **Publication Concentration**: [Description of how publications are distributed across journals]\n",
    "\n",
    "### 4. Content Analysis\n",
    "- **Common Themes**: [Key topics from title word frequency]\n",
    "- **Abstract Availability**: [Percentage] of papers have abstracts\n",
    "\n",
    "## Technical Implementation\n",
    "\n",
    "### Data Cleaning Challenges\n",
    "1. **Missing Values**: Handled missing titles, abstracts, and publication dates appropriately\n",
    "2. **Data Consistency**: Standardized journal names and publication formats\n",
    "3. **Text Processing**: Cleaned and normalized title text for analysis\n",
    "\n",
    "### Analysis Techniques\n",
    "1. **Time Series Analysis**: Tracked publication patterns over time\n",
    "2. **Text Mining**: Extracted key themes from paper titles\n",
    "3. **Comparative Analysis**: Compared publication output across different sources\n",
    "\n",
    "### Visualization Approach\n",
    "- Used multiple chart types (line, bar, word cloud) for comprehensive insights\n",
    "- Implemented interactive filters in Streamlit for user exploration\n",
    "- Ensured visualizations are clear and interpretable\n",
    "\n",
    "## Challenges Faced\n",
    "\n",
    "1. **Data Quality**: Inconsistent formatting in source data required careful cleaning\n",
    "2. **Scale Management**: Large dataset required efficient processing techniques\n",
    "3. **Visualization Selection**: Choosing the most effective charts for different data types\n",
    "\n",
    "## Lessons Learned\n",
    "\n",
    "1. **Data Preparation**: 80% of the work is in cleaning and preparing data\n",
    "2. **Iterative Analysis**: Multiple passes through data reveal different insights\n",
    "3. **User Experience**: Interactive tools significantly enhance data exploration\n",
    "\n",
    "## Future Enhancements\n",
    "\n",
    "1. **Advanced NLP**: Implement topic modeling on abstracts\n",
    "2. **Citation Analysis**: Include citation networks if available\n",
    "3. **Real-time Updates**: Connect to live data source\n",
    "4. **Collaboration Analysis**: Map author collaboration networks\n",
    "\n",
    "## Code Quality Features\n",
    "\n",
    "- **Modular Design**: Separate functions for loading, cleaning, analysis\n",
    "- **Comprehensive Documentation**: Clear comments and docstrings\n",
    "- **Error Handling**: Robust handling of edge cases\n",
    "- **Reproducibility**: Seed values for random operations\n",
    "\n",
    "## Conclusion\n",
    "This project successfully demonstrates a complete data analysis pipeline from raw data to interactive insights. The CORD-19 dataset provides valuable insights into the rapid research response to the COVID-19 pandemic.\n",
    "\"\"\"\n",
    "\n",
    "# Reflection on the project\n",
    "def project_reflection():\n",
    "    \"\"\"Reflect on the project challenges and learnings\"\"\"\n",
    "    \n",
    "    reflection = \"\"\"\n",
    "    PROJECT REFLECTION:\n",
    "    \n",
    "    SUCCESSES:\n",
    "    1. Complete pipeline implementation from data loading to interactive app\n",
    "    2. Comprehensive data cleaning and preparation\n",
    "    3. Multiple visualization types for different insights\n",
    "    4. User-friendly Streamlit interface\n",
    "    \n",
    "    CHALLENGES:\n",
    "    1. Handling large dataset efficiently\n",
    "    2. Dealing with inconsistent data quality\n",
    "    3. Choosing appropriate visualizations\n",
    "    4. Balancing complexity with usability\n",
    "    \n",
    "    KEY LEARNINGS:\n",
    "    1. Importance of thorough data exploration before analysis\n",
    "    2. Value of interactive tools for data discovery\n",
    "    3. Need for flexible data cleaning approaches\n",
    "    4. Benefits of modular code organization\n",
    "    \n",
    "    AREAS FOR IMPROVEMENT:\n",
    "    1. Add more advanced text analysis (sentiment, topics)\n",
    "    2. Implement performance optimizations for larger datasets\n",
    "    3. Add more interactive filtering options\n",
    "    4. Include statistical testing for observed patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    print(reflection)\n",
    "\n",
    "project_reflection("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21c17ab-a099-4f5c-83b9-0bcbe188b778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
